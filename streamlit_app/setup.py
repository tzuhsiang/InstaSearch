import zipfile
import os
import shutil
import json
import pandas as pd
import datetime
from elasticsearch import Elasticsearch
from contextlib import contextmanager
from typing import Dict, List
import logging

# å®šç¾©å¸¸æ•¸
BASE_DIR = "/app"
IG_DATA_DIR = os.path.join(BASE_DIR, "ig_data")
MEDIA_DIR = os.path.join(BASE_DIR, "media")
LOGS_DIR = os.path.join(BASE_DIR, "logs")
EXTRACT_PATH = os.path.join(IG_DATA_DIR, "tmp_extract")
POSTS_JSON_PATH = os.path.join(EXTRACT_PATH, "your_instagram_activity", "content", "posts_1.json")
ES_HOST = "http://elasticsearch:9200"
ES_INDEX = "ig_data"

# åˆå§‹åŒ–loggingï¼ˆå…ˆä¸å»ºç«‹æª”æ¡ˆï¼Œç­‰ç›®éŒ„æª¢æŸ¥å®Œæˆå¾Œå†è¨­å®šï¼‰
logger = logging.getLogger(__name__)
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.INFO)

@contextmanager
def elasticsearch_client():
    """å»ºç«‹Elasticsearchå®¢æˆ¶ç«¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    client = Elasticsearch(ES_HOST)
    try:
        yield client
    finally:
        client.close()

def check_directory_structure():
    """æª¢æŸ¥ä¸¦å»ºç«‹å¿…è¦çš„ç›®éŒ„çµæ§‹"""
    try:
        # é¦–å…ˆå‰µå»ºæ‰€æœ‰å¿…è¦çš„ç›®éŒ„
        directories = [IG_DATA_DIR, MEDIA_DIR, LOGS_DIR]
        for directory in directories:
            try:
                os.makedirs(directory, exist_ok=True)
                # ç¢ºä¿ç›®éŒ„æ¬Šé™
                os.chmod(directory, 0o777)
                logger.info(f"å»ºç«‹/æ›´æ–°ç›®éŒ„æ¬Šé™ï¼š{directory}")
            except Exception as e:
                raise PermissionError(f"ç„¡æ³•å‰µå»ºæˆ–ä¿®æ”¹ç›®éŒ„ {directory}: {e}")

        # æª¢æŸ¥æ¯å€‹ç›®éŒ„çš„å¯«å…¥æ¬Šé™
        for directory in directories:
            try:
                test_file = os.path.join(directory, '.test')
                with open(test_file, 'w') as f:
                    f.write('test')
                os.remove(test_file)
            except Exception as e:
                raise PermissionError(f"ç›®éŒ„ {directory} ç„¡æ³•å¯«å…¥: {e}")

        # ç¢ºèªæ‰€æœ‰ç›®éŒ„éƒ½å­˜åœ¨ä¸”å¯å¯«å…¥å¾Œï¼Œæ·»åŠ æª”æ¡ˆæ—¥èªŒè™•ç†å™¨
        file_handler = logging.FileHandler(os.path.join(LOGS_DIR, "setup.log"))
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
                
        logger.info("âœ… ç›®éŒ„çµæ§‹æª¢æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"âŒ ç›®éŒ„çµæ§‹æª¢æŸ¥å¤±æ•—: {e}")
        raise

def find_zip_file(zip_path: str = None) -> str:
    """å°‹æ‰¾zipæª”æ¡ˆ
    
    Args:
        zip_path: æŒ‡å®šçš„zipæª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœæœªæŒ‡å®šå‰‡å°‹æ‰¾ig_dataç›®éŒ„ä¸­å”¯ä¸€çš„zipæª”æ¡ˆ
    
    Returns:
        str: zipæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘
    
    Raises:
        FileNotFoundError: ç•¶æ‰¾ä¸åˆ°zipæª”æ¡ˆæ™‚
    """
    if zip_path and os.path.exists(zip_path):
        return zip_path
        
    if not os.path.exists(IG_DATA_DIR):
        raise FileNotFoundError(f"ç›®éŒ„ {IG_DATA_DIR} ä¸å­˜åœ¨")
    
    zip_files = [f for f in os.listdir(IG_DATA_DIR) if f.endswith('.zip')]
    if len(zip_files) != 1:
        raise FileNotFoundError("æ‰¾ä¸åˆ°å”¯ä¸€çš„ zip æª”æ¡ˆï¼Œè«‹ç¢ºèªç›®éŒ„ä¸­åªæœ‰ä¸€å€‹ zip æª”")
    
    return os.path.join(IG_DATA_DIR, zip_files[0])

def extract_zip(zip_path: str):
    """è§£å£“ç¸®Instagramè³‡æ–™æª”æ¡ˆ
    
    Args:
        zip_path: zipæª”æ¡ˆçš„è·¯å¾‘
    """
    logger.info(f"æ­£åœ¨è™•ç†ï¼š{zip_path}")
    try:
        # å‰µå»ºä¸¦è¨­ç½®è‡¨æ™‚ç›®éŒ„æ¬Šé™
        os.makedirs(EXTRACT_PATH, exist_ok=True)
        os.chmod(EXTRACT_PATH, 0o777)
        logger.info(f"å‰µå»ºè‡¨æ™‚ç›®éŒ„ï¼š{EXTRACT_PATH}")
        
        # è§£å£“æª”æ¡ˆ
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(EXTRACT_PATH)
        
        # è¨­ç½®è§£å£“å¾Œç›®éŒ„çš„æ¬Šé™
        for root, dirs, files in os.walk(EXTRACT_PATH):
            # è¨­ç½®ç›®éŒ„æ¬Šé™
            for d in dirs:
                dir_path = os.path.join(root, d)
                os.chmod(dir_path, 0o777)
            # è¨­ç½®æª”æ¡ˆæ¬Šé™
            for f in files:
                file_path = os.path.join(root, f)
                os.chmod(file_path, 0o666)
        
        logger.info("âœ… è§£å£“å®Œæˆä¸¦è¨­ç½®æ¬Šé™")
    except Exception as e:
        logger.error(f"è§£å£“æˆ–è¨­ç½®æ¬Šé™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        raise

def process_instagram_data() -> List[Dict]:
    """è™•ç†Instagram JSONè³‡æ–™
    
    Returns:
        List[Dict]: è™•ç†å¾Œçš„Instagramè³‡æ–™åˆ—è¡¨
    
    Raises:
        FileNotFoundError: ç•¶æ‰¾ä¸åˆ°posts_1.jsonæª”æ¡ˆæ™‚
    """
    if not os.path.exists(POSTS_JSON_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{POSTS_JSON_PATH}")
        
    with open(POSTS_JSON_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)

    content = []
    for item in data:
        try:
            # ä¿®æ”¹mediaè·¯å¾‘ï¼Œæ·»åŠ postså‰ç¶´
            media_list = item.get("media", [])
            processed_media = []
            for media_item in media_list:
                uri = media_item.get("uri", "")
                if uri:
                    # æå–æª”æ¡ˆåç¨±å’Œæ—¥æœŸç›®éŒ„
                    parts = uri.split('/')
                    filename = parts[-1]
                    date_dir = parts[-2] if len(parts) > 1 else None
                    
                    # çµ„åˆæ–°çš„è·¯å¾‘
                    if date_dir:
                        media_item["uri"] = os.path.join("media", "posts", date_dir, filename)
                    else:
                        media_item["uri"] = os.path.join("media", "posts", filename)
                processed_media.append(media_item)
                
            content.append({
                "media": processed_media,
                "title": item["title"].encode('latin1').decode('utf-8'),
                "creation_timestamp": datetime.datetime.fromtimestamp(item["creation_timestamp"]).isoformat()
            })
        except (KeyError, UnicodeError) as e:
            logger.warning(f"è™•ç†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            continue

    return content

def setup_elasticsearch_index():
    """è¨­ç½®Elasticsearchç´¢å¼•"""
    with elasticsearch_client() as es:
        if not es.ping():
            raise ConnectionError("ç„¡æ³•é€£æ¥åˆ°Elasticsearch")
        
        logger.info("âœ… æˆåŠŸé€£æ¥ Elasticsearch")

        # å¦‚æœç´¢å¼•å·²ç¶“å­˜åœ¨ï¼Œå‰‡åˆªé™¤
        if es.indices.exists(index=ES_INDEX):
            es.indices.delete(index=ES_INDEX)

        # å‰µå»ºæ–°ç´¢å¼•
        es.indices.create(index=ES_INDEX, body={
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "content": {"type": "text"},
                    "creation_timestamp": {"type": "date"}
                }
            }
        })
        logger.info(f"âœ… ç´¢å¼• '{ES_INDEX}' å·²å»ºç«‹")

def import_data_to_elasticsearch(data: List[Dict]):
    """å°‡è³‡æ–™å°å…¥Elasticsearch
    
    Args:
        data: è¦å°å…¥çš„è³‡æ–™åˆ—è¡¨
    """
    with elasticsearch_client() as es:
        for item in data:
            doc = {
                "content": item["title"],
                "datetime": item["creation_timestamp"],
                "timestamp": datetime.datetime.now().isoformat(),
                "media": item["media"]
            }
            res = es.index(index=ES_INDEX, body=doc)
            logger.info(f"âœ… æ–‡æœ¬å·²å¯«å…¥ï¼ŒID: {res['_id']}")

def copy_with_metadata(src: str, dst: str):
    """è¤‡è£½æª”æ¡ˆä¸¦ä¿ç•™metadata
    
    Args:
        src: ä¾†æºæª”æ¡ˆè·¯å¾‘
        dst: ç›®æ¨™æª”æ¡ˆè·¯å¾‘
    """
    # å»ºç«‹ç›®æ¨™ç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(os.path.dirname(dst), exist_ok=True)
    
    # ä½¿ç”¨ä½å±¤ç´šæª”æ¡ˆæ“ä½œä»¥ç¢ºä¿æ›´å¥½çš„æ§åˆ¶
    with open(src, 'rb') as fsrc:
        with open(dst, 'wb') as fdst:
            while True:
                chunk = fsrc.read(1024 * 1024)  # 1MB chunks
                if not chunk:
                    break
                fdst.write(chunk)
    
    # è¤‡è£½æª”æ¡ˆæ¬Šé™
    os.chmod(dst, 0o666)

def cleanup():
    """æ¸…ç†æš«å­˜æª”æ¡ˆå’Œç›®éŒ„"""
    # æ¬ç§»åª’é«”æª”æ¡ˆ
    media_dir = os.path.join(EXTRACT_PATH, "media")
    posts_dir = os.path.join(media_dir, "posts")
    if os.path.exists(posts_dir):
        try:
            # ç¢ºä¿ç›®æ¨™ç›®éŒ„å­˜åœ¨
            os.makedirs(MEDIA_DIR, exist_ok=True)
            os.chmod(MEDIA_DIR, 0o777)
            
            target_posts_dir = os.path.join(MEDIA_DIR, "posts")
            
            # å¦‚æœç›®æ¨™ç›®éŒ„å·²å­˜åœ¨ï¼Œå…ˆç§»é™¤
            if os.path.exists(target_posts_dir):
                for root, dirs, files in os.walk(target_posts_dir, topdown=False):
                    for name in files:
                        file_path = os.path.join(root, name)
                        try:
                            os.chmod(file_path, 0o666)
                            os.remove(file_path)
                        except OSError:
                            pass
                    for name in dirs:
                        dir_path = os.path.join(root, name)
                        try:
                            os.chmod(dir_path, 0o777)
                            os.rmdir(dir_path)
                        except OSError:
                            pass
                try:
                    os.rmdir(target_posts_dir)
                except OSError:
                    pass
            
            # å»ºç«‹ç›®æ¨™ç›®éŒ„
            os.makedirs(target_posts_dir, exist_ok=True)
            os.chmod(target_posts_dir, 0o777)
            
            # è¤‡è£½æ‰€æœ‰æª”æ¡ˆ
            for root, dirs, files in os.walk(posts_dir):
                # è¨ˆç®—ç›¸å°è·¯å¾‘
                rel_path = os.path.relpath(root, posts_dir)
                target_root = os.path.join(target_posts_dir, rel_path)
                
                # å»ºç«‹ç›®éŒ„
                os.makedirs(target_root, exist_ok=True)
                os.chmod(target_root, 0o777)
                
                # è¤‡è£½æª”æ¡ˆ
                for file in files:
                    src_file = os.path.join(root, file)
                    dst_file = os.path.join(target_root, file)
                    try:
                        copy_with_metadata(src_file, dst_file)
                        logger.debug(f"è¤‡è£½æª”æ¡ˆï¼š{src_file} -> {dst_file}")
                    except Exception as e:
                        logger.warning(f"è¤‡è£½æª”æ¡ˆå¤±æ•— {src_file}: {e}")
                        continue
            
            logger.info("è¤‡è£½å®Œæˆä¸¦è¨­ç½®æ¬Šé™")
        except Exception as e:
            logger.error(f"ç§»å‹•æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            raise
        
        logger.info(f"ğŸ“ è³‡æ–™å·²æ¬ç§»åˆ° {MEDIA_DIR} ä¸¦è¨­ç½®é©ç•¶æ¬Šé™")
    
    # æ¸…é™¤æš«å­˜
    if os.path.exists(EXTRACT_PATH):
        shutil.rmtree(EXTRACT_PATH)
    
    # æ¸…é™¤JSONæª”æ¡ˆ
    json_file = os.path.join(IG_DATA_DIR, "ig_data.json")
    if os.path.exists(json_file):
        os.remove(json_file)
        logger.info(f"å·²åˆªé™¤ï¼š{json_file}")

def process_instagram_zip(zip_path: str = None) -> tuple[bool, str]:
    """è™•ç†Instagram ZIPæª”æ¡ˆçš„ä¸»è¦å‡½æ•¸
    
    Args:
        zip_path: æŒ‡å®šçš„zipæª”æ¡ˆè·¯å¾‘ï¼Œå¦‚æœæœªæŒ‡å®šå‰‡å°‹æ‰¾ig_dataç›®éŒ„ä¸­å”¯ä¸€çš„zipæª”æ¡ˆ
        
    Returns:
        tuple[bool, str]: (æ˜¯å¦æˆåŠŸ, éŒ¯èª¤è¨Šæ¯)
    """
    try:
        # æª¢æŸ¥ç›®éŒ„çµæ§‹å’Œæ¬Šé™
        check_directory_structure()
        
        zip_path = find_zip_file(zip_path)
        extract_zip(zip_path)
        
        data = process_instagram_data()
        
        # å°‡è™•ç†å¾Œçš„è³‡æ–™æš«å­˜ç‚ºJSON
        with open(os.path.join(IG_DATA_DIR, "ig_data.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info("List å·²æˆåŠŸå­˜åˆ° ig_data.json æª”æ¡ˆä¸­ï¼")
        
        setup_elasticsearch_index()
        import_data_to_elasticsearch(data)
        
        cleanup()
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")
        
        return True, None
        
    except Exception as e:
        error_msg = f"ç¨‹åºåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        logger.error(error_msg)
        return False, error_msg

if __name__ == "__main__":
    process_instagram_zip()

import zipfile
import os
import shutil
import json
import pandas as pd
import datetime
from elasticsearch import Elasticsearch
from contextlib import contextmanager
from typing import Dict, List
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å®šç¾©å¸¸æ•¸
BASE_DIR = "."
IG_DATA_DIR = os.path.join(BASE_DIR, "ig_data")
MEDIA_DIR = os.path.join(BASE_DIR, "media")
EXTRACT_PATH = os.path.join(IG_DATA_DIR, "tmp_extract")
POSTS_JSON_PATH = os.path.join(EXTRACT_PATH, "your_instagram_activity", "content", "posts_1.json")
ES_HOST = "http://localhost:9200"
ES_INDEX = "ig_data"

@contextmanager
def elasticsearch_client():
    """å»ºç«‹Elasticsearchå®¢æˆ¶ç«¯çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    client = Elasticsearch(ES_HOST)
    try:
        yield client
    finally:
        client.close()

def find_zip_file() -> str:
    """å°‹æ‰¾ig_dataç›®éŒ„ä¸­å”¯ä¸€çš„zipæª”æ¡ˆ
    
    Returns:
        str: zipæª”æ¡ˆçš„å®Œæ•´è·¯å¾‘
    
    Raises:
        FileNotFoundError: ç•¶æ‰¾ä¸åˆ°zipæª”æ¡ˆæˆ–æœ‰å¤šå€‹zipæª”æ¡ˆæ™‚
    """
    zip_files = [f for f in os.listdir(IG_DATA_DIR) if f.endswith('.zip')]
    if len(zip_files) != 1:
        raise FileNotFoundError("æ‰¾ä¸åˆ°å”¯ä¸€çš„ zip æª”æ¡ˆï¼Œè«‹ç¢ºèªç›®éŒ„ä¸­åªæœ‰ä¸€å€‹ zip æª”")
    
    return os.path.join(IG_DATA_DIR, zip_files[0])

def extract_zip(zip_path: str):
    """è§£å£“ç¸®Instagramè³‡æ–™æª”æ¡ˆ
    
    Args:
        zip_path: zipæª”æ¡ˆçš„è·¯å¾‘
    """
    logger.info(f"æ­£åœ¨è™•ç†ï¼š{zip_path}")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(EXTRACT_PATH)

def process_instagram_data() -> List[Dict]:
    """è™•ç†Instagram JSONè³‡æ–™
    
    Returns:
        List[Dict]: è™•ç†å¾Œçš„Instagramè³‡æ–™åˆ—è¡¨
    """
    with open(POSTS_JSON_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)

    content = []
    for item in data:
        try:
            content.append({
                "media": item["media"],
                "title": item["title"].encode('latin1').decode('utf-8'),
                "creation_timestamp": datetime.datetime.fromtimestamp(item["creation_timestamp"]).isoformat()
            })
        except (KeyError, UnicodeError) as e:
            logger.warning(f"è™•ç†è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            continue

    return content

def setup_elasticsearch_index():
    """è¨­ç½®Elasticsearchç´¢å¼•"""
    with elasticsearch_client() as es:
        if not es.ping():
            raise ConnectionError("ç„¡æ³•é€£æ¥åˆ°Elasticsearch")
        
        logger.info("âœ… æˆåŠŸé€£æ¥ Elasticsearch")

        # å¦‚æœç´¢å¼•å·²ç¶“å­˜åœ¨ï¼Œå‰‡åˆªé™¤
        if es.indices.exists(index=ES_INDEX):
            es.indices.delete(index=ES_INDEX)

        # å‰µå»ºæ–°ç´¢å¼•
        es.indices.create(index=ES_INDEX, body={
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0
            },
            "mappings": {
                "properties": {
                    "content": {"type": "text"},
                    "creation_timestamp": {"type": "date"}
                }
            }
        })
        logger.info(f"âœ… ç´¢å¼• '{ES_INDEX}' å·²å»ºç«‹")

def import_data_to_elasticsearch(data: List[Dict]):
    """å°‡è³‡æ–™å°å…¥Elasticsearch
    
    Args:
        data: è¦å°å…¥çš„è³‡æ–™åˆ—è¡¨
    """
    with elasticsearch_client() as es:
        for item in data:
            doc = {
                "content": item["title"],
                "datetime": item["creation_timestamp"],
                "timestamp": datetime.datetime.now().isoformat(),
                "media": item["media"]
            }
            res = es.index(index=ES_INDEX, body=doc)
            logger.info(f"âœ… æ–‡æœ¬å·²å¯«å…¥ï¼ŒID: {res['_id']}")

def cleanup():
    """æ¸…ç†æš«å­˜æª”æ¡ˆå’Œç›®éŒ„"""
    # æ¬ç§»åª’é«”æª”æ¡ˆ
    selected_subdir = os.path.join(EXTRACT_PATH, "media")
    if os.path.exists(selected_subdir):
        os.makedirs(MEDIA_DIR, exist_ok=True)
        if os.path.exists(MEDIA_DIR):
            shutil.rmtree(MEDIA_DIR)
        shutil.move(selected_subdir, MEDIA_DIR)
        logger.info(f"ğŸ“ è³‡æ–™å·²æ¬ç§»åˆ° {MEDIA_DIR}")
    
    # æ¸…é™¤æš«å­˜
    if os.path.exists(EXTRACT_PATH):
        shutil.rmtree(EXTRACT_PATH)
    
    # æ¸…é™¤JSONæª”æ¡ˆ
    json_file = os.path.join(IG_DATA_DIR, "ig_data.json")
    if os.path.exists(json_file):
        os.remove(json_file)
        logger.info(f"å·²åˆªé™¤ï¼š{json_file}")

def main():
    """ä¸»ç¨‹åº"""
    try:
        zip_path = find_zip_file()
        extract_zip(zip_path)
        
        data = process_instagram_data()
        
        # å°‡è™•ç†å¾Œçš„è³‡æ–™æš«å­˜ç‚ºJSON
        with open(os.path.join(IG_DATA_DIR, "ig_data.json"), "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info("List å·²æˆåŠŸå­˜åˆ° ig_data.json æª”æ¡ˆä¸­ï¼")
        
        setup_elasticsearch_index()
        import_data_to_elasticsearch(data)
        
        cleanup()
        logger.info("âœ… åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"ç¨‹åºåŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        raise

if __name__ == "__main__":
    main()
